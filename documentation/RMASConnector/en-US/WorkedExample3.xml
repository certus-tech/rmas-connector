<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [
<!ENTITY % BOOK_ENTITIES SYSTEM "RMASConnector.ent">
%BOOK_ENTITIES;
]>
<chapter id="chap-RMASConnector-WorkedExample3">
  <title>Worked Example 3: Key mapping across multiple tools</title>

  <section id="sect-RMASConnector-WorkedExample3-Section_1">
    <title>Overview</title>

    <para>Worked example 3 demonstrates how data from multiple sources can be joined even when sources maintain different identifying fields (keys). For example, one may wish to aggregate personnel
    across Human Resources (HR) and project management databases.</para>

    <para>Assuming that the keys for data entries in one source of interest are different from those in another, this example demonstrates how a key mapping file can be utilised. Such a file maintains
    relationship between keys in different data sets.</para>
  </section>

  <section id="sect-RMASConnector-WorkedExample3-Section_2">
    <title>User Story</title>

    <para>A project manager wishes to find the contact details of staff who have released publications in the last twelve months.</para>

    <para>They have been provided with a Symplectic export file (a CSV file) which lists publications from the last twelve months, where the authors are identified by keys which are unique to
    Symplectic. They also have access to the HR database, where each member of staff is identified by a unique HR key. Finally, they have a mapping file (again in the form of a CSV file), which
    identifies which key in the HR database relates to map which key in Symplectic.</para>

    <para>The project manager starts PDI and opens the Key Mapping job. They modify the job and transformation parameters to specify the details of the HR database, the locations of their Symplectic
    and key mapping CSV files, and the location of an output file. They run the job, which joins the publication data with HR data, generating .a CSV file with the required information.</para>
  </section>

  <section id="sect-RMASConnector-WorkedExample3-Section_3">
    <title>Structure of the Example</title>

    <para>Worked example 3 consists of two PDI components: a job (WorkedExample3.kjb) and a transformation (MapAndAggregateData.ktr). These are located in the WorkedExample3 subdirectory of the PDI
    repository.</para>

    <para>Worked example 3 may be used in the context of the Spoon graphical front end to PDI, by loading the job and executing it directly. It may also be executed on the command line.</para>

    <section id="sect-RMASConnector-WorkedExample3-Section_3_1">
      <title>Parameters</title>

      <para>The following parameters are defined in the <emphasis>WorkedExample3</emphasis> job and used by the transformations:</para>

      <table id="sect-RMASConnector-WorkedExample3-Section_3_1_table">
        <caption></caption>

        <thead>
          <tr>
            <td>Parameter</td>

            <td>Description</td>
          </tr>
        </thead>

        <tbody>
          <tr>
            <td>CSVFile</td>

            <td>The path to the CSV file containing publication data.</td>
          </tr>

          <tr>
            <td>DatabaseClass</td>

            <td>The database driver class name.</td>
          </tr>

          <tr>
            <td>DatabaseUrl</td>

            <td>The URL for the database connection.</td>
          </tr>

          <tr>
            <td>DatabaseUsername</td>

            <td>The username for accessing the database.</td>
          </tr>

          <tr>
            <td>DatabasePassword</td>

            <td>The password for accessing the database.</td>
          </tr>

          <tr>
            <td>SQLQuery</td>

            <td>The query to execute on the database.</td>
          </tr>

          <tr>
            <td>mapping.file</td>

            <td>The path to the CSV file containing key mappings.</td>
          </tr>

          <tr>
            <td>output.file</td>

            <td>The path to the file which is generated by the process. If the file does not exist, it will be created automatically.</td>
          </tr>

          <tr>
            <td>project.dir</td>

            <td>The root directory for the project.</td>
          </tr>
        </tbody>
      </table>
    </section>
  </section>

  <section id="sect-RMASConnector-WorkedExample3-Section_4">
    <title>Running the job from Spoon</title>

    <para>This section describes how to configure and/or run the job from the <emphasis>Spoon </emphasis>visual editor.</para>

    <section id="sect-RMASConnector-WorkedExample3-Section_4_1">
      <title>Job Structure</title>

      <figure id="sect-RMASConnector-WorkedExample3-Section_4_1_figure">
        <title>The worked example 3 job</title>

        <mediaobject>
          <imageobject condition="web">
            <imagedata align="center" fileref="images/WorkedExample3/Example3Job.png" scalefit="0" />
          </imageobject>

          <textobject>
            <phrase>Worked Example 3 Job</phrase>
          </textobject>
        </mediaobject>
      </figure>

      <para>The Worked example 3 job uses a single transformation, <emphasis>MapAndAggregateData</emphasis>, to perform the key mapping and join.</para>
    </section>

    <section id="sect-RMASConnector-WorkedExample3-Section_4_2">
      <title>Aggregate Data</title>

      <para>The <emphasis>MapAndAggregateData</emphasis> transformation uses two streams. The top stream performs a query on the HR database, and then removes fields which are not required, while
      mapping others to a CERIF-backed Common Data Model (CDM). The rows are then sorted ready for joining:</para>

      <figure id="sect-RMASConnector-WorkedExample3-Section_4_2_figure">
        <title>The MapAndAggregateData transformation top stream</title>

        <mediaobject>
          <imageobject condition="web">
            <imagedata align="center" fileref="images/WorkedExample3/Example3TopStream.png" />
          </imageobject>

          <textobject>
            <phrase>The MapAndAggregateData transformation top stream</phrase>
          </textobject>
        </mediaobject>
      </figure>

      <para>As an alternative, the fields removed by the <emphasis>Map To CDM</emphasis> step could have been omitted by altering the SQLQuery parameter.</para>

      <para>The bottom stream extracts data from a Symplectic CSV file. It removes unwanted fields and maps the remainder to the CDM. A lookup step is then performed, where the key for each row in the
      Symplectic data is looked up in the mapping CSV file, and the corresponding HR key is added to the row:</para>

      <figure id="sect-RMASConnector-WorkedExample3-Section_4_3_figure">
        <title>The MapAndAggregateData transformation bottom stream</title>

        <mediaobject>
          <imageobject condition="web">
            <imagedata align="center" fileref="images/WorkedExample3/Example3BottomStream.png" scalefit="0" />
          </imageobject>

          <textobject>
            <phrase>The MapAndAggregateData transformation bottom stream</phrase>
          </textobject>
        </mediaobject>
      </figure>

      <para>Prior to the join, the data in the stream is again sorted on the HR key. This is necessary in order to correctly perform a merge join on the HR key:</para>

      <figure id="sect-RMASConnector-WorkedExample3-Section_4_4_figure">
        <title>The MapAndAggregateData transformation join</title>

        <mediaobject>
          <imageobject condition="web">
            <imagedata align="center" fileref="images/WorkedExample3/Example3Join.png" scalefit="0" />
          </imageobject>

          <textobject>
            <phrase>The MapAndAggregateData transformation join</phrase>
          </textobject>
        </mediaobject>
      </figure>

      <para>The joined data is then saved to a CSV file.</para>
    </section>

    <section>
      <title>A note on scalability</title>

      <para>The previous section describes how data can be aggregated from two sources. In order for this transformation to aggregate data from additional sources, a new stream with a mapping step
      must be added for each additional source. Because a merge step only allows two inputs, new merge steps need to be added to merge the data from each new stream with the merged data from the
      inital two streams. As such, the complexity of this transformation increases for each additional source.</para>
    </section>
  </section>

  <section id="sect-RMASConnector-WorkedExample3-Section_5">
    <title>Running the job from the command line</title>

    <para>You can also run the job from the command line, either as the downloaded worked example or after configuring and saving it using Spoon. The following shows how to run the command from both a
    linux environment and a Windows environment, using the <emphasis>Kitchen</emphasis> tool which is provided with PDI.</para>

    <section id="sect-RMASConnector-WorkedExample3-Section_5_1">
      <title>Running the job from a Linux command line</title>

      <para>Navigate to the RMAS/kettle directory: <programlisting>cd RMAS/kettle</programlisting></para>

      <para>Ensure that the kitchen.sh script is executable: <programlisting>chmod u+x kitchen.sh</programlisting></para>

      <para>Run the job using the following command. Note that the quote marks around the repository name are optional, but they <emphasis>must</emphasis> be included if the repository name contains
      whitespace: <programlisting>./kitchen.sh -rep="RMASConnector" -job=WorkedExample3/WorkedExample3</programlisting></para>
    </section>

    <section id="sect-RMASConnector-WorkedExample3-Section_5_2">
      <title>Running the job from a Windows command line</title>

      <para>Navigate to the RMAS/kettle directory: <programlisting>cd RMAS\kettle</programlisting></para>

      <para>Run the job using the following command. Note that the quote marks around the repository name are optional, but they <emphasis>must</emphasis> be included if the repository name contains
      whitespace: <programlisting>Kitchen.bat /rep:"RMASConnector" /job:WorkedExample3/WorkedExample3</programlisting></para>
    </section>
  </section>

  <section id="sect-RMASConnector-WorkedExample3-Section_6">
    <title>A note on configuring the worked example</title>

    <para>This example requires a connection to a Human Resources database. This can be configured by ensuring that all the database parameters are set at the job level. Default values for these
    parameters have been entered for the sample database which is provided as part of the example (RMAS/exampledata/WorkedExample3/WorkedExample3.script). When configuring your own database, you will
    need to ensure that the <emphasis>Map to CDM</emphasis> step in the <emphasis>MapAndAggregateData</emphasis> transformation maps the column names in the database to the appropriate CDM
    fields.</para>

    <para>A CSV file of the required publication must be supplied. A sample has been provided as a part of the example and can be found at RMAS/exampledata/WorkedExample3/SymplecticInput.csv. The path
    to the files must be set as the value of the CSVFile parameter at the job level. As with the database, you should ensure that the <emphasis>Map Symplectic to CDM</emphasis> step maps the
    publication data to the appropriate fields.</para>

    <para>Finally, a CSV mapping file is required. This identifies which keys in the publication data map to which in the HR database. The column headers used for this are “mapping_key” and “HR_key”
    respectively. An example file is provided at RMAS/exampledata/WorkedExample3/SymplecticKeyMapping.csv.</para>

    <para>On running the job, the joined data is output to a file in the location specified by the output.file parameter. An outer join is performed, so if there is Symplectic data with no
    corresponding HR data, or vice versa, it will still appear in the output.</para>
  </section>
</chapter>
